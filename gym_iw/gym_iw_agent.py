import argparse
import sys
import os

sys.path.append(os.path.dirname(os.path.realpath(__file__)))

from __gym_iw import __GymIwAgent

import gym
from gym import wrappers, logger

class GymIwAgent(__GymIwAgent):
    """GYM-IW agent"""
    def __init__(self,
                 environment,
                 planner,
                 encoding='byte',
                 space_relative_precision=0.001,
                 frameskip=1,
                 simulator_budget=150000,
                 time_budget=float('inf'),
                 novelty_subtables=False,
                 random_actions=False,
                 max_rep=30,
                 discount=1.0,
                 nodes_threshold=50000,
                 break_ties_using_rewards=False,
                 max_depth=1500,
                 debug_threshold=0,
                 random_seed=0,
                 logger_mode='info'):
        '''Constructor of Gym IW agents
        
        Parameters:
        environment (gym.Env): Gym environment
        planner (str): IW planner ('rollout-iw' or 'bfs-iw')
        encoding (str): Gym space encoding to feature atoms ('byte' or 'variable')
        space_relative_precision (float): relative precision of gym space variable-based encoding
        frameskip (int): frame skip rate
        simulator_budget (int): budget for #calls to simulator for online decision making
        time_budget (float): time budget for online decision making
        novelty_subtables (bool): use of single novelty table or novelty subtables
        random_actions (bool): use of random action when there are no rewards in look-ahead tree 
        max_rep (int): max rep(etition) of features during lookahead
        discount (float): discount factor for lookahead
        nodes_threshold (int): threshold in #nodes for expanding look-ahead tree
        break_ties_using_rewards (bool): break ties in favor of better rewards during bfs-iw
        max_depth (int): max depth for lookahead
        debug_threshold (int): threshold for debug mode
        random_seed (int): random seed
        logger_mode (str): logger mode ('debug' or 'info' or 'warning' or 'error' or 'stats' or 'silent')

        '''
        super().__init__(environment=environment, planner=planner, encoding=encoding, space_relative_precision=space_relative_precision, 
                         frameskip=frameskip, simulator_budget=simulator_budget, time_budget=time_budget, novelty_subtables=novelty_subtables,
                         random_actions=random_actions, max_rep=max_rep, discount=discount, nodes_threshold=nodes_threshold,
                         break_ties_using_rewards=break_ties_using_rewards, max_depth=max_depth, debug_threshold=debug_threshold,
                         random_seed=random_seed, logger_mode=logger_mode)
    
    def get_number_of_observation_feature_atoms(self):
        '''Get the number of observation feature atoms generated by the chosen encoding mode (set in __init__)
        
        Returns:
        int: number of observation feature atoms
        '''
        return super().get_number_of_observation_feature_atoms()
    
    def get_number_of_action_feature_atoms(self):
        '''Get the number of action feature atoms generated by the chosen encoding mode (set in __init__)
        
        Note: action feature atoms are not needed by the IW algorithms per se but it
        helps generate a finite set of actions in case of continuous action
        space (Gym specificity)
        
        Returns:
        int: number of action feature atoms
        '''
        return super().get_number_of_action_feature_atoms()
    
    def play(self,
             episodes=1,
             initial_random_noops=1,
             lookahead_caching=2,
             prefix_length_to_execute=0.0,
             execute_single_action=False,
             max_execution_length_in_frames=18000):
        '''Play a sequence of episodes

        Parameters:
        episodes (int): number of episodes
        initial_random_noops (int): max number of initial noops, actual number is sampled (must be greater than 0)
        lookahead_caching (int): lookahead caching (0=none, 1=partial, 2=full)
        prefix_length_to_execute (float): % of prefix to execute (default is 0 = execute until positive reward)
        execute_single_action (bool): execute only one action from best branch in lookahead (default is to execute prefix until first reward)
        max_execution_length_in_frames (int): max number of frames in single execution
        
        '''
        super().play(episodes=episodes, initial_random_noops=initial_random_noops, lookahead_caching=lookahead_caching,
                     prefix_length_to_execute=prefix_length_to_execute, execute_single_action=execute_single_action,
                     max_execution_length_in_frames=max_execution_length_in_frames)

    def start_episode(self, lookahead_caching=2):
        '''Start an episode in interactive mode (MUST be called before calling several act() functions)
        
        Parameters:
        lookahead_caching (int): lookahead caching (0=none, 1=partial, 2=full)

        '''
        super().start_episode(lookahead_caching=lookahead_caching)
    
    def act(self, observation, reward, done):
        '''Select the (planned) action to execute
        
        Parameters:
        observation (Gym space observation element): current Gym observation
        reward (float): last reward received from the Gym environment
        done (bool): whether the Gym environment has reached a terminal state

        '''
        return super().act(observation=observation, reward=reward, done=done)
    
    def end_episode(self):
        '''End an episode in interactive mode (MUST be called after calling several act() functions)
        
        '''
        return super().end_episode()

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description=None)
    parser.add_argument('env_id', nargs='?', default='CartPole-v0', help='Select the environment to run')
    parser.add_argument('planner', nargs='?', default='bfs-iw', help='Select the planner to run (rollout-iw or bfs-iw)')
    parser.add_argument('encoding', nargs='?', default='byte', help='Select the gym space encoding to feature atoms (byte or variable)')
    parser.add_argument('space_relative_precision', nargs='?', default=0.001, help='Select the relative precision of gym space variable-based encoding')
    args = parser.parse_args()

    # You can set the level to logger.DEBUG or logger.WARN if you
    # want to change the amount of output.
    logger.set_level(logger.INFO)

    env = gym.make(args.env_id)

    # You provide the directory to write to (can be an existing
    # directory, including one with existing data -- all monitor files
    # will be namespaced). You can also dump to a tempdir if you'd
    # like: tempfile.mkdtemp().
    outdir = '/tmp/gym-iw-agent-results'
    #env = wrappers.Monitor(env, directory=outdir, force=True)
    env.seed(0)
    agent = GymIwAgent(environment=env, planner=args.planner, encoding=args.encoding, space_relative_precision=args.space_relative_precision)

    episode_count = 10

    for i in range(episode_count):
        reward = 0
        done = False
        
        ob = env.reset()
        env.render()
        env.close()
        agent.start_episode()

        while True:
            action = agent.act(ob, reward, done)
            ob, reward, done, _ = env.step(action)
            env.render()
            env.close()
            if done:
                break
            # Note there's no env.render() here. But the environment still can open window and
            # render if asked by env.monitor: it calls env.render('rgb_array') to record video.
            # Video is not recorded every episode, see
            # capped_cubic_video_schedule for details.
        agent.end_episode()
        

    # Close the env and write monitor result info to disk
    env.close()
